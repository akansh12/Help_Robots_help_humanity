{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c0cd2a1fd3c9b2e6a6e9f61c8d8b36e46f1682ab"
   },
   "source": [
    "## LSTM for time-series\n",
    "\n",
    "In this competition we basically have time-series, so it makes sense to solve it as such.\n",
    "\n",
    "In this kernel I build an LSTM neural net in Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader,Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "import altair as alt\n",
    "from altair.vega import v3\n",
    "from IPython.display import HTML\n",
    "import json\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torch.utils.data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, train_test_split, GroupKFold, GroupShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "712895963a877392ba25583198111d80876a30a6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "requirejs.config({\n",
       "    baseUrl: 'https://cdn.jsdelivr.net/npm/',\n",
       "    paths: {\"vega\": \"https://cdn.jsdelivr.net/npm/vega@v3.3.1?noext\", \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\", \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@v2.6.0?noext\", \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@3?noext\"}\n",
       "});\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preparing altair. I use code from this great kernel: https://www.kaggle.com/notslush/altair-visualization-2018-stackoverflow-survey\n",
    "\n",
    "vega_url = 'https://cdn.jsdelivr.net/npm/vega@' + v3.SCHEMA_VERSION\n",
    "vega_lib_url = 'https://cdn.jsdelivr.net/npm/vega-lib'\n",
    "vega_lite_url = 'https://cdn.jsdelivr.net/npm/vega-lite@' + alt.SCHEMA_VERSION\n",
    "vega_embed_url = 'https://cdn.jsdelivr.net/npm/vega-embed@3'\n",
    "noext = \"?noext\"\n",
    "\n",
    "paths = {\n",
    "    'vega': vega_url + noext,\n",
    "    'vega-lib': vega_lib_url + noext,\n",
    "    'vega-lite': vega_lite_url + noext,\n",
    "    'vega-embed': vega_embed_url + noext\n",
    "}\n",
    "\n",
    "workaround = \"\"\"\n",
    "requirejs.config({{\n",
    "    baseUrl: 'https://cdn.jsdelivr.net/npm/',\n",
    "    paths: {}\n",
    "}});\n",
    "\"\"\"\n",
    "\n",
    "#------------------------------------------------ Defs for future rendering\n",
    "def add_autoincrement(render_func):\n",
    "    # Keep track of unique <div/> IDs\n",
    "    cache = {}\n",
    "    def wrapped(chart, id=\"vega-chart\", autoincrement=True):\n",
    "        if autoincrement:\n",
    "            if id in cache:\n",
    "                counter = 1 + cache[id]\n",
    "                cache[id] = counter\n",
    "            else:\n",
    "                cache[id] = 0\n",
    "            actual_id = id if cache[id] == 0 else id + '-' + str(cache[id])\n",
    "        else:\n",
    "            if id not in cache:\n",
    "                cache[id] = 0\n",
    "            actual_id = id\n",
    "        return render_func(chart, id=actual_id)\n",
    "    # Cache will stay outside and \n",
    "    return wrapped\n",
    "            \n",
    "@add_autoincrement\n",
    "def render(chart, id=\"vega-chart\"):\n",
    "    chart_str = \"\"\"\n",
    "    <div id=\"{id}\"></div><script>\n",
    "    require([\"vega-embed\"], function(vg_embed) {{\n",
    "        const spec = {chart};     \n",
    "        vg_embed(\"#{id}\", spec, {{defaultStyle: true}}).catch(console.warn);\n",
    "        console.log(\"anything?\");\n",
    "    }});\n",
    "    console.log(\"really...anything?\");\n",
    "    </script>\n",
    "    \"\"\"\n",
    "    return HTML(\n",
    "        chart_str.format(\n",
    "            id=id,\n",
    "            chart=json.dumps(chart) if isinstance(chart, dict) else chart.to_json(indent=None)\n",
    "        )\n",
    "    )\n",
    "\n",
    "HTML(\"\".join((\n",
    "    \"<script>\",\n",
    "    workaround.format(json.dumps(paths)),\n",
    "    \"</script>\",\n",
    ")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ee31cdd4e99d65a1472fb29518a00589471f01e9"
   },
   "source": [
    "## Loading and preparing data\n",
    "\n",
    "We need to convert data from csv into 3D array containing series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "c3738f8d34b447d1391bb13385516bc1447e1b04"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/X_train.csv')\n",
    "test = pd.read_csv('../input/X_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "9719cce4f4c7cee587d79cf0b09fd57730e7c6f1"
   },
   "outputs": [],
   "source": [
    "for col in train.columns:\n",
    "    if 'orient' in col:\n",
    "        scaler = StandardScaler()\n",
    "        train[col] = scaler.fit_transform(train[col].values.reshape(-1, 1))\n",
    "        test[col] = scaler.transform(test[col].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "339c7f432d20238ed75397a331dc55f3ffda65c4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3810, 10, 128)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = np.array([x.values[:,3:].T for group,x in train.groupby('series_id')], dtype='float32')\n",
    "test = np.array([x.values[:,3:].T for group,x in test.groupby('series_id')], dtype='float32')\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "20f5d8d8c2dbce02dcc516bdcf4be13c9d8c7fcd"
   },
   "source": [
    "### Encoding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "7f7cc7959dd775cb15f2111b74bff0487a4693e5"
   },
   "outputs": [],
   "source": [
    "y = pd.read_csv('../input/y_train.csv')\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y['surface'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "09ce140968d03b8ab46b848b97bd764f350224eb"
   },
   "source": [
    "### LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "a19b36d2d433f67cec005279a06be20af4b6d5cc"
   },
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, hidden_dim, num_layers, dropout, bidirectional, num_classes, batch_size):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_dir = 2 if bidirectional else 1\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=self.in_dim, hidden_size=self.hidden_dim, num_layers=self.num_layers, dropout=self.dropout/2, bidirectional=self.bidirectional,\n",
    "                            batch_first=True)\n",
    "        self.gru = nn.GRU(self.hidden_dim * 2, self.hidden_dim, bidirectional=self.bidirectional, batch_first=True)\n",
    "\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(4096, int(hidden_dim)),\n",
    "            nn.SELU(True),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(int(hidden_dim), num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x.permute(2, 0, 1)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        gru_out, _ = self.gru(lstm_out)\n",
    "        avg_pool_l = torch.mean(lstm_out.permute(1, 0, 2), 1)\n",
    "        max_pool_l, _ = torch.max(lstm_out.permute(1, 0, 2), 1)\n",
    "        \n",
    "        avg_pool_g = torch.mean(gru_out.permute(1, 0, 2), 1)\n",
    "        max_pool_g, _ = torch.max(gru_out.permute(1, 0, 2), 1)\n",
    "\n",
    "        x = torch.cat((avg_pool_g, max_pool_g, avg_pool_l, max_pool_l), 1)\n",
    "        y = self.fc(x)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "85ce154b9996b27f937f88943604649e6b905632"
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "46f988015e0c7769fd7ffd76760cd8ed56a5fff3"
   },
   "outputs": [],
   "source": [
    "def plot_training_process(df: pd.DataFrame(), epoch_col: str, value_columns: list, y_axis_name: str, title: str):\n",
    "    \n",
    "    # code mostly based on: https://altair-viz.github.io/gallery/multiline_tooltip.html\n",
    "    plot_df = df.melt(id_vars=epoch_col, value_vars=value_columns, var_name='group', value_name=y_axis_name)\n",
    "    plot_df[y_axis_name] = plot_df[y_axis_name].round(4)\n",
    "    nearest = alt.selection(type='single', nearest=True, on='mouseover', fields=[epoch_col], empty='none')\n",
    "    line = alt.Chart().mark_line(interpolate='basis').encode(\n",
    "        x=f'{epoch_col}:Q',\n",
    "        y=f'{y_axis_name}:Q',\n",
    "        color='group:N',\n",
    "    ).properties(\n",
    "        title=title\n",
    "    )\n",
    "\n",
    "    # Transparent selectors across the chart. This is what tells us\n",
    "    # the x-value of the cursor\n",
    "    selectors = alt.Chart().mark_point().encode(\n",
    "        x=f'{epoch_col}:Q',\n",
    "        opacity=alt.value(0),\n",
    "    ).add_selection(\n",
    "        nearest\n",
    "    )\n",
    "\n",
    "    # Draw points on the line, and highlight based on selection\n",
    "    points = line.mark_point().encode(\n",
    "        opacity=alt.condition(nearest, alt.value(1), alt.value(0))\n",
    "    )\n",
    "\n",
    "    # Draw text labels near the points, and highlight based on selection\n",
    "    text = line.mark_text(align='left', dx=5, dy=-5).encode(\n",
    "        text=alt.condition(nearest, f'{y_axis_name}:Q', alt.value(' '))\n",
    "    )\n",
    "\n",
    "    # Draw a rule at the location of the selection\n",
    "    rules = alt.Chart().mark_rule(color='gray').encode(\n",
    "        x=f'{epoch_col}:Q',\n",
    "    ).transform_filter(\n",
    "        nearest\n",
    "    )\n",
    "\n",
    "    # Put the five layers into a chart and bind the data\n",
    "    return alt.layer(line, selectors, points, rules, text,\n",
    "              data=plot_df, width=600, height=300).interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "281f2723df6f32decfe2175e92c156aec3359d8f"
   },
   "outputs": [],
   "source": [
    "def train_net(train_loader, val_loader, patience, model, criterion, optimizer, scheduler, verbose, plot_training):\n",
    "    valid_loss_min = np.Inf\n",
    "    patience = patience\n",
    "    # current number of epochs, where validation loss didn't increase\n",
    "    p = 0\n",
    "    # whether training should be stopped\n",
    "    stop = False\n",
    "\n",
    "    epochs = 20000\n",
    "    training_logs = []\n",
    "\n",
    "    for e in range(1, epochs + 1):\n",
    "        # print(time.ctime(), 'Epoch:', e)\n",
    "        train_loss = []\n",
    "        train_acc = []\n",
    "        model.train()\n",
    "        for batch_i, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "            a = target.data.cpu().numpy()\n",
    "            b = output.detach().cpu().numpy().argmax(1)\n",
    "            train_acc.append(accuracy_score(a, b))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        val_loss = []\n",
    "        val_acc = []\n",
    "        for batch_i, (data, target) in enumerate(val_loader):\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            output = model(data)\n",
    "\n",
    "            loss = criterion(output, target)\n",
    "            val_loss.append(loss.item()) \n",
    "            a = target.data.cpu().numpy()\n",
    "            b = output.detach().cpu().numpy().argmax(1)\n",
    "            val_acc.append(accuracy_score(a, b))\n",
    "\n",
    "        if e % 100 == 0 and verbose:\n",
    "            print(f'Epoch {e}, train loss: {np.mean(train_loss):.4f}, valid loss: {np.mean(val_loss):.4f}, train acc: {np.mean(train_acc):.4f}, valid acc: {np.mean(val_acc):.4f}')\n",
    "\n",
    "        training_logs.append([e, np.mean(train_loss), np.mean(val_loss), np.mean(train_acc), np.mean(val_acc)])\n",
    "\n",
    "        scheduler.step(np.mean(val_loss))\n",
    "\n",
    "        valid_loss = np.mean(val_loss)\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            # print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min, valid_loss))\n",
    "            torch.save(model.state_dict(), 'model.pt')\n",
    "            valid_loss_min = valid_loss\n",
    "            p = 0\n",
    "\n",
    "        # check if validation loss didn't improve\n",
    "        if valid_loss > valid_loss_min:\n",
    "            p += 1\n",
    "            # print(f'{p} epochs of increasing val loss')\n",
    "            if p > patience:\n",
    "                print('Stopping training')\n",
    "                stop = True\n",
    "                break        \n",
    "\n",
    "        if stop:\n",
    "            break\n",
    "\n",
    "    checkpoint = torch.load('model.pt')      \n",
    "    model.load_state_dict(checkpoint)\n",
    "    \n",
    "    if plot_training:\n",
    "        training_logs = pd.DataFrame(training_logs, columns=['Epoch', 'Train loss', 'Valid loss', 'Train accuracy', 'Validation accuracy'])\n",
    "        loss_plot = plot_training_process(df=training_logs, epoch_col='Epoch', value_columns=['Train loss', 'Valid loss'], y_axis_name='loss', title='Loss progress')\n",
    "        acc_plot = plot_training_process(df=training_logs, epoch_col='Epoch', value_columns=['Train accuracy', 'Validation accuracy'], y_axis_name='accuracy', title='Accuracy progress')\n",
    "        render(loss_plot & acc_plot)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "89a0776d59505a320ffee1fb505c97f21f91b5d6"
   },
   "outputs": [],
   "source": [
    "def initialize_model():\n",
    "    torch.manual_seed(42)\n",
    "    model = LSTMClassifier(10, 512, 3, 0.1, True, 9, 64)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=.5)\n",
    "    model.cuda()\n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, patience=8, factor=0.5, verbose=True)\n",
    "    return model, criterion, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "255d52ae34b4a0471912bcc15423445367834545"
   },
   "outputs": [],
   "source": [
    "def train_net_folds(X, X_test, y, folds, plot_training, batch_size, patience, verbose, le):\n",
    "\n",
    "    oof = np.zeros((len(X), 9))\n",
    "    prediction = np.zeros((len(X_test), 9))\n",
    "    scores = []\n",
    "    for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):\n",
    "        print('Fold', fold_n, 'started at', time.ctime())\n",
    "        X_train, X_valid = X[train_index], X[valid_index]\n",
    "        y_train, y_valid = y[train_index], y[valid_index]\n",
    "        \n",
    "        train_set = torch.utils.data.TensorDataset(torch.FloatTensor(X_train), torch.LongTensor(y_train))\n",
    "        val_set = torch.utils.data.TensorDataset(torch.FloatTensor(X_valid), torch.LongTensor(y_valid))\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(train_set,batch_size=batch_size, shuffle=True)\n",
    "        val_loader = torch.utils.data.DataLoader(val_set,batch_size=batch_size)\n",
    "        \n",
    "        model, criterion, optimizer, scheduler = initialize_model()\n",
    "        model = train_net(train_loader, val_loader, patience, model, criterion, optimizer, scheduler, verbose, plot_training)\n",
    "        \n",
    "        y_pred_valid = []\n",
    "        for batch_i, (data, target) in enumerate(val_loader):\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            p = model(data)\n",
    "            pred = p.cpu().detach().numpy()\n",
    "            y_pred_valid.extend(pred)\n",
    "            \n",
    "        y_pred = []\n",
    "        for i, data in enumerate(test):\n",
    "            p = model(torch.FloatTensor(data).unsqueeze(0).cuda())\n",
    "            y_pred.append(p.cpu().detach().numpy().flatten())\n",
    "            \n",
    "        oof[valid_index] = np.array(y_pred_valid)\n",
    "        scores.append(accuracy_score(y_valid, np.array(y_pred_valid).argmax(1)))\n",
    "\n",
    "        prediction += y_pred\n",
    "\n",
    "    prediction /= n_fold\n",
    "    \n",
    "    prediction = np.array(prediction).argmax(1)\n",
    "    \n",
    "    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n",
    "    print('--' * 50)\n",
    "    \n",
    "    return oof, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "93bfcc903b1ef04a18d0032bea9cf831f257aead",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Thu Apr 11 02:22:10 2019\n",
      "Epoch 100, train loss: 0.7957, valid loss: 0.9663, train acc: 0.7063, valid acc: 0.6451\n",
      "Epoch   110: reducing learning rate of group 0 to 2.5000e-01.\n",
      "Epoch   124: reducing learning rate of group 0 to 1.2500e-01.\n",
      "Epoch   135: reducing learning rate of group 0 to 6.2500e-02.\n",
      "Epoch   158: reducing learning rate of group 0 to 3.1250e-02.\n",
      "Epoch   167: reducing learning rate of group 0 to 1.5625e-02.\n",
      "Epoch   182: reducing learning rate of group 0 to 7.8125e-03.\n",
      "Epoch   191: reducing learning rate of group 0 to 3.9062e-03.\n",
      "Epoch 200, train loss: 0.2424, valid loss: 0.7375, train acc: 0.9153, valid acc: 0.7990\n",
      "Epoch   200: reducing learning rate of group 0 to 1.9531e-03.\n",
      "Epoch   209: reducing learning rate of group 0 to 9.7656e-04.\n",
      "Epoch   221: reducing learning rate of group 0 to 4.8828e-04.\n",
      "Epoch   230: reducing learning rate of group 0 to 2.4414e-04.\n",
      "Epoch   239: reducing learning rate of group 0 to 1.2207e-04.\n",
      "Epoch   248: reducing learning rate of group 0 to 6.1035e-05.\n",
      "Stopping training\n",
      "Fold 1 started at Thu Apr 11 02:59:50 2019\n",
      "Epoch    66: reducing learning rate of group 0 to 2.5000e-01.\n",
      "Epoch    95: reducing learning rate of group 0 to 1.2500e-01.\n",
      "Epoch 100, train loss: 0.7375, valid loss: 0.8556, train acc: 0.7311, valid acc: 0.6813\n",
      "Epoch   124: reducing learning rate of group 0 to 6.2500e-02.\n",
      "Epoch   136: reducing learning rate of group 0 to 3.1250e-02.\n",
      "Epoch   147: reducing learning rate of group 0 to 1.5625e-02.\n",
      "Epoch   157: reducing learning rate of group 0 to 7.8125e-03.\n",
      "Epoch   166: reducing learning rate of group 0 to 3.9062e-03.\n",
      "Epoch   175: reducing learning rate of group 0 to 1.9531e-03.\n",
      "Epoch   184: reducing learning rate of group 0 to 9.7656e-04.\n",
      "Stopping training\n",
      "Fold 2 started at Thu Apr 11 03:28:03 2019\n",
      "Epoch    68: reducing learning rate of group 0 to 2.5000e-01.\n",
      "Epoch    84: reducing learning rate of group 0 to 1.2500e-01.\n",
      "Epoch 100, train loss: 0.7282, valid loss: 0.8243, train acc: 0.7421, valid acc: 0.7032\n",
      "Epoch   104: reducing learning rate of group 0 to 6.2500e-02.\n",
      "Epoch   125: reducing learning rate of group 0 to 3.1250e-02.\n",
      "Epoch   148: reducing learning rate of group 0 to 1.5625e-02.\n",
      "Epoch   175: reducing learning rate of group 0 to 7.8125e-03.\n",
      "Epoch   188: reducing learning rate of group 0 to 3.9062e-03.\n",
      "Epoch   197: reducing learning rate of group 0 to 1.9531e-03.\n",
      "Epoch 200, train loss: 0.5270, valid loss: 0.6998, train acc: 0.8258, valid acc: 0.7663\n",
      "Epoch   206: reducing learning rate of group 0 to 9.7656e-04.\n",
      "Epoch   217: reducing learning rate of group 0 to 4.8828e-04.\n",
      "Epoch   226: reducing learning rate of group 0 to 2.4414e-04.\n",
      "Epoch   235: reducing learning rate of group 0 to 1.2207e-04.\n",
      "Epoch   253: reducing learning rate of group 0 to 6.1035e-05.\n",
      "Epoch   262: reducing learning rate of group 0 to 3.0518e-05.\n",
      "Epoch   271: reducing learning rate of group 0 to 1.5259e-05.\n",
      "Epoch   280: reducing learning rate of group 0 to 7.6294e-06.\n",
      "Stopping training\n",
      "Fold 3 started at Thu Apr 11 04:10:27 2019\n",
      "Epoch    77: reducing learning rate of group 0 to 2.5000e-01.\n",
      "Epoch 100, train loss: 0.7494, valid loss: 1.0960, train acc: 0.7307, valid acc: 0.6116\n",
      "Epoch   128: reducing learning rate of group 0 to 1.2500e-01.\n",
      "Epoch   141: reducing learning rate of group 0 to 6.2500e-02.\n",
      "Epoch   152: reducing learning rate of group 0 to 3.1250e-02.\n",
      "Epoch   180: reducing learning rate of group 0 to 1.5625e-02.\n",
      "Epoch   190: reducing learning rate of group 0 to 7.8125e-03.\n",
      "Epoch 200, train loss: 0.3203, valid loss: 0.5678, train acc: 0.8953, valid acc: 0.8183\n",
      "Epoch   208: reducing learning rate of group 0 to 3.9062e-03.\n",
      "Epoch   217: reducing learning rate of group 0 to 1.9531e-03.\n",
      "Epoch   226: reducing learning rate of group 0 to 9.7656e-04.\n",
      "Epoch   235: reducing learning rate of group 0 to 4.8828e-04.\n",
      "Stopping training\n",
      "Fold 4 started at Thu Apr 11 04:46:14 2019\n",
      "Epoch    74: reducing learning rate of group 0 to 2.5000e-01.\n",
      "Epoch    90: reducing learning rate of group 0 to 1.2500e-01.\n",
      "Epoch 100, train loss: 0.6484, valid loss: 0.8950, train acc: 0.7678, valid acc: 0.7009\n",
      "Epoch   106: reducing learning rate of group 0 to 6.2500e-02.\n",
      "Epoch   140: reducing learning rate of group 0 to 3.1250e-02.\n",
      "Epoch   172: reducing learning rate of group 0 to 1.5625e-02.\n",
      "Epoch   186: reducing learning rate of group 0 to 7.8125e-03.\n",
      "Epoch   195: reducing learning rate of group 0 to 3.9062e-03.\n",
      "Epoch 200, train loss: 0.3663, valid loss: 0.7485, train acc: 0.8779, valid acc: 0.7772\n",
      "Epoch   211: reducing learning rate of group 0 to 1.9531e-03.\n",
      "Epoch   220: reducing learning rate of group 0 to 9.7656e-04.\n",
      "Epoch   229: reducing learning rate of group 0 to 4.8828e-04.\n",
      "Epoch   238: reducing learning rate of group 0 to 2.4414e-04.\n",
      "Stopping training\n",
      "CV mean score: 0.7759, std: 0.0214.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "n_fold = 5\n",
    "folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)\n",
    "oof, prediction = train_net_folds(train, test, y, folds, True, 64, 40, True, le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "6ba4ee95fc74550eefea186f54a0b97562b2d2f8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>surface</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hard_tiles_large_space</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>carpet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tiled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>soft_tiles</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  surface\n",
       "0  hard_tiles_large_space\n",
       "1                  carpet\n",
       "2                   tiled\n",
       "3                    wood\n",
       "4              soft_tiles"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub = pd.DataFrame(le.inverse_transform(prediction), columns=['surface'])\n",
    "sub.to_csv('submission.csv', index_label='series_id')\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "d34d7bd376ed7fb894de80052091ff4d15ddc202"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
